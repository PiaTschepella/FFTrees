---
title: "Accuracy Statistics in FFTrees"
author: "Nathaniel Phillips and Hansjörg Neth"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: fft.bib
csl: apa.csl 
vignette: >
  %\VignetteIndexEntry{Accuracy Statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7.5, fig.height = 7.5, dpi = 100, out.width = "600px", fig.align='center', message = FALSE)
```


```{r, echo = FALSE, message = FALSE, results = 'hide'}
library(FFTrees)
```


## Accuracy Statistics in **FFTrees** 

In this vignette, we cover how accuracy statistics are calculated for FFTs and the **FFTrees** package [as described in @phillips2017FFTrees]. 
Most of these measures are not specific to FFTs and can be used for any classification algorithm. 

First, let's look at the accuracy statistics from a heart disease FFT: 

```{r fft-example, out.width="50%", fig.align = "center", echo = FALSE, fig.cap = "**Figure 1**: Example FFT for the `heartdisease` data."}
# Create an FFTrees object predicting heart disease
heart.fft <- FFTrees(formula = diagnosis ~.,
                     data = heartdisease)

plot(heart.fft)
```

You'll notice a 2\ x\ 2 table in the bottom-left hand side of the plot. 
This is a _2\ x\ 2 matrix_ or _confusion table_ (see [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) or [Neth et al., 2021](https://doi.org/10.3389/fpsyg.2020.567817) for details). 
A wide range of accuracy measures can be derived from this simple matrix. 
Here is a generic version of a confusion table: 

```{r confusion-table, fig.align = "center", out.width="50%", echo = FALSE, fig.cap = "**Figure 2**: A 2x2 matrix illustrating the frequency counts of 4 possible outcomes."}
knitr::include_graphics("../inst/confusiontable.jpg")
```

The 2\ x\ 2 matrix cross-tabulates the decisions of the algorithm (rows) with actual criterion values (columns) and contains counts of observations for all four resulting cells. 
Counts in cells\ a and\ d refer to correct decisions due to the match between predicted and criterion values, whereas counts in cells\ b and\ c refer to errors due to the mismatch between predicted and criterion values. 
Both correct decisions and errors come in two types: 

- Cell *hi* represents hits, positive criterion values correctly predicted to be positive, and cell *cr* represents correct rejections, negative criterion values correctly predicted to be negative. 

- As for errors, cell *fa* represents false alarms, negative criterion values erroneously predicted to be positive, and cell *mi* represents misses, positive criterion values erroneously predicted to be negative. 

Given this structure, an accurate decision algorithm aims to maximize the frequencies in cells\ *hi* and\ *cr* while minimizing those in cells\ *fa* and\ *mi*. 


| Output | Description                   | Formula | 
|:-------|:------------------------------|:----------------------|
|   hi |  Number of hits   | $N(Decision = 1 \land Truth = 1)$ |
|   mi |  Number of misses | $N(Decision = 0 \land Truth = 1)$ |
|   fa |  Number of false-alarms | $N(Decision = 1 \land Truth = 0)$ |
|   cr |  Number of correct rejections | $N(Decision = 0 \land Truth = 0)$ |
|    n |  Total number of cases| $hi + mi + fa + cr$ |

**Table 1**: Definitions of raw frequencies in a confusion table. The notation $N()$ means number of cases (or frequency counts). 


### Conditional accuracy statistics

The first set of accuracy statistics are based on subsets of the data, conditional on either algorithm decisions (positive predictive value and negative predictive value), or criterion values (sensitivity and specificity). In other words, they are based on either rows or columns of the confusion table:


| Output | Description | Formula | 
|:------ |:------------------------------|:----------------------|
| sens   | Sensitivity  | $p(Decision = 1 \vert Truth = 1) = hi / (hi + mi)$ |
| spec   | Specificity  | $p(Decision = 0 \vert Truth = 0) = cr / (cr + fa)$ |
| far    | False alarm rate  | $1 - spec$ |
| ppv    | Positive predictive value  | $p(Truth = 1 \vert Decision = 1) = hi / (hi + fa)$ |
| npv    | Negative predictive value  | $p(Truth = 0 \vert Decision = 0) = cr / (cr + mi)$ |

**Table 2**: Conditional accuracy statistics based on either rows or columns of the confusion table.


The _sensitivity_ (aka. _hit-rate_) is defined as $sens = hi/(hi+mi)$ and represents the percentage of cases with positive criterion values that were correctly predicted by the algorithm. 
Similarly, _specificity_ (aka. _correct rejection rate_, or the complement of the _false alarm rate_) is defined as $spec = cr/(fa + cr)$ and represents the percentage of cases with negative criterion values correctly predicted by the algorithm.

The _positive-predictive value_ $ppv$ and _negative predictive value_ $npv$ are the flip-sides of $sens$ and $spec$, as they are conditional accuracies based on decision outcomes (rather than on true criterion values). 


### Aggregate accuracy statistics

Additional accuracy statistics are based on all four cells in the confusion table: 

| Output | Description | Formula | 
|:------ |:------------------------------|:----------------------|
| acc    | Accuracy | $(hi + cr) / (hi + mi + fa + cr)$ |
| bacc   | Balanced accuracy  | $sens \times .5 + spec \times .5$ |
| wacc   | Weighted accuracy  | $sens \times w + spec \times w$   |
| bpv    | Balanced predictive value  | $ppv \times .5 + npv \times .5$ |
| dprime | D-prime | $zsens - zfar$ |

**Table 3**: Aggreagte accuracy statistics based on all four cells of the confusion table.

Overall accuracy (`acc`) is defined as the overall percentage of correct decisions ignoring the difference between hits and correct rejections. $bacc$ and $wacc$ are averages of sensitivity and specificity, whlie $bpv$ is an average of predictive value. $dprime$ is the difference in standardized (z-score) transformed $sens$ and $far$


### Speed and frugality statistics

The next two statistics measure the speed and frugality of a fast-and-frugal tree. Unlike the accuracy statistics above, they are *not* based on the confusion table. Rather, they depend on how much information the trees use to make decisions.

| Output| Description| Formula | 
|:------|:-----------------------------------------|:----------------------|
| mcu   | Mean cues used: Average number of cue values used in making classifications, averaged across all cases | |
| pci   | Percentage of cues ignored: Percentage of cues ignored when classifying cases | $N(CuesInData) - mcu$ |

To see exactly where these statistics come from, let's look at the results for `heart.fft` (FFT \#1): 

```{r fft-heart}
heart.fft
```

According to this output, FFT \#1 has $mcu = 1.73$ and $pci = 0.88$. 
We can easily calculate these measures directly from the `x$levelout` output from an `FFTrees` object. 
This object contains the level (i.e., node) where each case was classified:

```{r fft-levelout}
# A vector of nodes at which each case was classified in FFT #1:
heart.fft$trees$decisions$train$tree_1$levelout
```

Now, to calculate\ $mcu$ (the _mean number of cues used_), we simply take the mean of this vector:

```{r fft-mcu}
# Calculate the mean number or cues used (mcu): 
mean(heart.fft$trees$decisions$train$tree_1$levelout)
```

Now that we know where\ $mcu$ comes from, computing $pci$ (i.e., the _percentage of cues ignored_) is just as simple --- it's just the total number of cues in the dataset minus\ $mcu$, divided by the total number of cues in the data: 

```{r fft-pci}
# Calculate pci (percentage of cues ignored) as 
# (N.Cues - mcu) / N.Cues):

(ncol(heartdisease) - heart.fft$trees$stats$train$mcu[1]) / ncol(heartdisease)
```

<!-- Reference for further measures: -->

### Additional measures 

For additional measures based on the frequency counts of a 2x2 matrix, see [Table 3](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.567817/full#h5) of [@neth2021matrix]: 

- Neth, H., Gradwohl, N., Streeb, D., Keim, D.A., & Gaissmaier, W. (2021). 
Perspectives on the 2×2 matrix: Solving semantically distinct problems based on a shared structure of binary contingencies. _Frontiers in Psychology_, _11_, 567817. doi: [10.3389/fpsyg.2020.567817](https://doi.org/10.3389/fpsyg.2020.567817)


## Vignettes

<!-- Table of all vignettes: -->

Here is a complete list of the vignettes available in the **FFTrees** package: 

|   | Vignette | Description |
|--:|:------------------------------|:-------------------------------------------------|
|   | [Main guide](guide.html) | An overview of the **FFTrees** package |
| 1 | [Heart Disease Tutorial](FFTrees_heart.html)   | An example of using `FFTrees()` to model heart disease diagnosis |
| 2 | [Accuracy statistics](AccuracyStatistics.html) | Definitions of accuracy statistics used throughout the package |
| 3 | [Creating FFTs with FFTrees()](FFTrees_function.html) | Details on the main function `FFTrees()` |
| 4 | [Specifying FFTs directly](FFTrees_mytree.html)   | How to directly create FFTs with `my.tree` without using the built-in algorithms |
| 5 | [Visualizing FFTs with plot()](FFTrees_plot.html) | Plotting `FFTrees` objects, from full trees to icon arrays |
| 6 | [Examples of FFTs](FFTrees_examples.html) | Examples of FFTs from different datasets contained in the package |


## References

<!-- eof. -->
