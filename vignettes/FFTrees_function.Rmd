---
title: "Creating FFTrees with FFTrees()"
author: "Nathaniel Phillips"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: fft.bib
csl: apa.csl 
vignette: >
  %\VignetteIndexEntry{Creating FFTrees with FFTrees()}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7.5, fig.height = 7.5, dpi = 100, out.width = "600px", fig.align='center', message = FALSE)
```

```{r load-pkg, echo = FALSE, message = FALSE, results = 'hide'}
library(FFTrees)
```


## Details on the `FFTrees()` function 

This vignette starts by building fast-and-frugal tree (FFTs) from the `heartdisease` data --- also used in the [Heart Disease Tutorial](FFTrees_heart.html) and @phillips2017FFTrees --- but then delves deeper into the details of the `FFTrees()` function. 

The `FFTrees()` function is at the heart of the **FFTrees** package. 
It takes a training dataset as an argument, and generates several FFTs which attempt to classify cases into one of two classes (i.e., a criterion variable that can either be True or False) based on cues (aka. features or predictor variables). 


### Example: Predicting heart disease

```{r image, fig.align = "center", out.width="250px", echo = FALSE}
knitr::include_graphics("../inst/CoronaryArtery.jpg")
```

We first create some FFTs for the `heartdisease` diagnosis data. 
The full dataset is provided as `heartdisease` in the **FFTrees** package. 
For modeling purposes, we have split the data into a training set (`heart.train`), and test set (`heart.test`). 
Here is how the corresponding data frames look: 

```{r heart-data}
# Training data: 
head(heartdisease)

# Testing data:
head(heartdisease)
```

The critical dependent variable (or binary criterion variable) is `diagnosis`. This variable indicates whether a patient has heart disease (`diagnosis = TRUE`) or not (`diagnosis = FALSE`). All other variables in the dataset (e.g., `sex`, `age`, and several biological measurements) can be used as predictors (aka. cues). 


### Creating trees with `FFTrees()`

To illustrate the difference between fitting and prediction, we will train the FFTs on `heart.train`, and test their prediction performance in `heart.test`. 
Note that we can also automate the training / test split using the `train.p` argument in `FFTrees()`. 
Setting `train.p` will randomly split `train.p`\% of the original data into a training set. 

To create a set of FFTs, we use the `FFTrees()` function to create a new `FFTrees` object called `heart.fft`. 
Here, we specify `diagnosis` as the binary criterion (or dependent variable), and include all other (independent) variables with `formula = diagnosis ~ .`: 

```{r heart-fft, message = FALSE}
# Create an FFTrees object called heart.fft predicting diagnosis: 
heart.fft <- FFTrees(formula = diagnosis ~.,
                     data = heart.train,
                     data.test = heart.test)
```

- If we wanted to only consider specific variables, like `sex` and `age`, for our trees, we could specify `formula = diagnosis ~ age + sex`. 


### Elements of an `FFTrees` object

The `FFTrees()` function returns an object of the `FFTrees` class. 
There are many elements in an `FFTrees` object. Here are their names:

```{r print-names}
# Print the names of the elements of an FFTrees object:
names(heart.fft)
```

<!-- ToDo: The following list seems outdated: -->

- `formula`: The formula used to create the FFTrees object.
- `data.desc`: Basic information about the datasets.
- `cue.accuracies`: Thresholds and marginal accuracies for each cue.
- `tree.definitions`: Definitions of all trees in the object.
- `tree.stats`: Classification statistics for all trees (tree definitions are also included here).
- `level.stats`: Cumulative classification statistics for each level of each tree.
- `decision`: Classification decisions for each case (row) for each tree (column).
- `levelout`: The level at which each case (row) is classified for each tree (column).
- `auc`: Area under the curve statistics
- `params`: Parameters used in tree construction
- `comp`: Models and statistics for alternative classification algorithms.

We can view basic information about the `FFTrees` object by printing its name. 
The default tree construction algorithm `ifan` creates multiple trees with different exit structures. 
When printing an `FFTrees` object, we automatically obtain the information about the tree with the highest value of the `goal` statistic. By default, the `goal` is set to weighed accuracy\ `wacc`:

```{r print-fftrees-object}
# Printing an FFTrees object shows details of the best tree, given the current goal:
heart.fft
```

Here is a description of the statistics provided:

| Statistic| Long name | Definition |
|:----- |:---------|:----------------------------------|
| `n`   | N        | The number of cases considered    |
| `mcu` | Mean cues used | On average, how many cues were needed to classify cases? 
In other words, what percent of the available information was used on average? |
| `pci` | Percent cues ignored | The percent of data that was _ignored_ when classifying cases with a given tree. 
This is identical to `mcu / cues.n`, where `cues.n` is the total number of cues in the data. |
| `sens`| Sensitivity | The percentage (or conditional probability) of true positive cases correctly classified. |
| `spec`| Specificity | The percentage (or conditional probability) of true negative cases correctly classified. |
| `acc` | Accuracy    | The percentage of cases that were correctly classified. |
| `wacc`| Weighted accuracy | The weighted average of sensitivity and specificity, 
where sensitivity is weighted by `sens.w` (by default, `sens.w = .5`). |


### Cue accuracy statistics 

Each tree has a decision threshold for each cue (regardless of whether or not it is actually used in the tree) that maximizes the `goal` value of that cue when it is applied to the entire training dataset. 
We can obtain cue accuracy statistics using the calculated decision thresholds from the `cue.accuracies` list. 
If the object has test data, we can see the marginal cue accuracies in the test data (using the thresholds calculated from the training data):

```{r fft-cues-stats-train}
# Decision thresholds and marginal classification training accuracies for each cue: 
heart.fft$cues$stats$train
```

We can also view the cue accuracies in an ROC\ plot with `plot()` combined with the `what = "cues"` argument. 
This will show the sensitivities and specificities for each cue, with the top five cues highlighted:

```{r fft-plot-cues, fig.width = 6.5, fig.height = 6.5, dpi = 400, out.width = "600px", fig.align='center'}
# Visualize individual cue accuracies: 
plot(heart.fft, 
     main = "Heartdisease cue accuracy",
     what = "cues")
```


### Tree definitions

The `tree.definitions` data frame contains definitions (cues, classes, exits, thresholds, and directions) of all trees in an `FFTrees` object. 
The combination of these five pieces of information (as well as their order), define and describe _how_ a tree makes decisions: 

```{r fft-definitions}
# Print the definitions of all trees
heart.fft$trees$definitions
```

To understand how to read these definitions, let's start by understanding tree `r heart.fft$trees$best$train`, the tree with the highest training weighted accuracy: 

Separate levels in tree definitions are separated by colons `;`. 
For example, Tree\ #3 has three cues in the order `thal`, `cp`, `ca`. 
The classes of the cues are\ `c` (character), `c` and\ `n` (numeric). 
The decision exits for the cues are 1\ (positive), 0\ (negative), and\ 0.5 (both positive and negative). 
This means that the first cue only makes positive decisions, the second cue only makes negative decisions, and the third cue makes _both_ positive and negative decisions. 

The decision thresholds are `rd` and `fd` for the first cue, `a` for the second cue, and `0` for the third cue while the cue directions are `=` for the first cue, `=` for the second cue, and `>` for the third cue. 
Note that cue directions indicate how the tree _would_ make positive decisions _if_ it had a positive exit for that cue. If the tree has a positive exit for the given cue, then cases that satisfy this threshold and direction are classified as positive. However, if the tree has only a negative exit for a given cue, then cases that do _not_ satisfy the given thresholds are classified as negative. 

From this information, we can understand and verbalize Tree\ \#3 as follows: 

1. If `thal` is equal to either `rd` or `fd`, predict a positive criterion value. 
2. Otherwise, if `cp` is not equal to `a`, predict a negative value. 
3. Otherwise, if `ca` is greater than\ 0, predict a positive value,  
else predict a negative value. 

We can use the `inwords()` function to automatically return a verbal description of the tree with the highest training accuracy (i.e., Tree\ #1) of an `FFTrees` object: 

```{r fft-inwords}
# Describe the best training tree (#1):
inwords(heart.fft, tree = 1)
```


### Accuracy statistics

The performance of an FFT on a specific dataset is characterized by a range of accuracy statistics. 
Here are the training statistics for all trees in `heart.fft`: 

```{r fft-stats-train}
# Training statistics for all trees:
heart.fft$trees$stats$train
```

The corresponding statistics for the testing are:

```{r fft-stats-test}
# Testing statistics for all trees:
heart.fft$trees$stats$test
```

See the [Accuracy statistics](AccuracyStatistics.html) vignette for the definitions of accuracy statistics used throughout the **FFTrees** package. 


### Classification decisions

The `decision` list contains the raw classification decisions for each tree and each case.

Here are is how decisions were made based on Tree\ #1: 

```{r fft-decisions-1}
# Inspect decisisions of Tree #1:
heart.fft$trees$decisions$train$tree_1
```


### Predicting new cases with `predict()`

Once we have created an `FFTrees` object, we can use it to predict new data using `predict()`. 
In this example, we'll use the `heart.fft` object to make predictions for cases\ 1 through\ 10 of the `heartdisease` dataset. 
By default, the tree with the best training\ `wacc` values is used to predict the value of the binary criterion variable: 

```{r fft-predict-class}
# Predict classes for new data from the best training tree: 
predict(heart.fft,
        newdata = heartdisease[1:10, ])
```

To predict class probabilities, we can include the `type = "prob"` argument. 
This will return a matrix of class predictions, where the first column indicates the probabilities for a case being classified as\ 0 / `FALSE`, and the second column indicates the probability for a case being classified as\ 1 / `TRUE`:

```{r fft-predict-prob}
# Predict class probabilities for new data from the best training tree:
predict(heart.fft,
        newdata = heartdisease,
        type = "prob")
```

Use `type = "both"` to get both classification and probability predictions for cases:

```{r fft-predict-both}
# Predict both classes and probabilities:
predict(heart.fft,
        newdata = heartdisease,
        type = "both")
```


### Visualising trees

Once we have created an `FFTrees` object using the `FFTrees()` function we can visualize the tree (and ROC\ curves) using `plot()`. The following code will visualize the best training tree applied to the test data: 

```{r fft-plot, fig.width = 7, fig.height = 7}
plot(heart.fft,
     main = "Heart Disease",
     decision.labels = c("Healthy", "Disease"))
```

See the [Plotting FFTrees objects](FFTrees_plot.html) vignette for more details on visualizing trees. 


### Manually defining an FFT 

We can also define a specific FFT to apply to a dataset using the `my.tree` argument. 
To do so, we specify the FFT as a sentence, making sure to correctly spell the cue names as they appear in the data. Sets of factor cues can be specified using (curly) brackets. 
For example, we can manually define an FFT by using the sentence 

- `"If chol > 300, predict True. If thal = {fd,rd}, predict False. Otherwise, predict True"`

```{r fft-my-tree}
# Manually define a tree using the my.tree argument:
myheart.fft <- FFTrees(diagnosis ~., 
                       data = heartdisease, 
                       my.tree = "If chol > 300, predict True. 
                                  If thal = {fd,rd}, predict False. 
                                  Otherwise, predict True")

# Here is the result
plot(myheart.fft, 
     main = "Specifying an FFT manually")
```

As we can see, the performance of this particular tree is pretty terrible --- but this allows you to easily build better FFTs yourself. 

See the [Specifying FFTs directly](FFTrees_mytree.html) vignette for more details on specifying FFTs from verbal descriptions. 


## Vignettes

<!-- Table of all vignettes: -->

Here is a complete list of the vignettes available in the **FFTrees** package: 

|   | Vignette | Description |
|--:|:------------------------------|:-------------------------------------------------|
|   | [Main guide](guide.html) | An overview of the **FFTrees** package |
| 1 | [Heart Disease Tutorial](FFTrees_heart.html)   | An example of using `FFTrees()` to model heart disease diagnosis |
| 2 | [Accuracy statistics](AccuracyStatistics.html) | Definitions of accuracy statistics used throughout the package |
| 3 | [Creating FFTs with FFTrees()](FFTrees_function.html) | Details on the main function `FFTrees()` |
| 4 | [Specifying FFTs directly](FFTrees_mytree.html)   | How to directly create FFTs with `my.tree` without using the built-in algorithms |
| 5 | [Visualizing FFTs with plot()](FFTrees_plot.html) | Plotting `FFTrees` objects, from full trees to icon arrays |
| 6 | [Examples of FFTs](FFTrees_examples.html) | Examples of FFTs from different datasets contained in the package |


## References 

<!-- eof. -->
